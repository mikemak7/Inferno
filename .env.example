# =============================================================================
# INFERNO-AI CONFIGURATION
# =============================================================================
# Copy this file to .env and configure your settings
# cp .env.example .env

# =============================================================================
# AUTHENTICATION
# =============================================================================
# Choose ONE authentication method:

# Option 1: API Key (pay per token) - ALL PLATFORMS
# Get your key at: https://console.anthropic.com/settings/keys
# ANTHROPIC_API_KEY=sk-ant-api03-...

# Option 2: Claude Code OAuth (macOS only - FREE with subscription)
# If you have Claude Code CLI installed and logged in, Inferno automatically
# reuses your Claude subscription from macOS Keychain. Just run: claude login
# No configuration needed here!

# Option 3: OAuth Token via Environment Variable (WINDOWS/LINUX - FREE)
# For users without macOS Keychain, set the OAuth token directly.
# Get your token from Claude Code CLI or Inferno OAuth flow.
# CLAUDE_CODE_OAUTH_TOKEN=sk-ant-oat01-...

# Option 4: Inferno OAuth Flow (all platforms - FREE with subscription)
# Run: inferno auth login
# This opens your browser to authenticate with your Claude subscription

# =============================================================================
# MODEL CONFIGURATION
# =============================================================================

# Claude model to use
# Options:
#   - claude-opus-4-5-20251101    (most capable, recommended for complex tasks)
#   - claude-sonnet-4-5-20250929  (balanced performance/cost)
#   - claude-haiku-4-5-20251001   (fastest, lowest cost)
INFERNO_MODEL__MODEL_ID=claude-opus-4-5-20251101

# Maximum tokens in response (1-200000)
INFERNO_MODEL__MAX_TOKENS=4096

# Temperature for response creativity (0.0-1.0)
# Lower = more focused, Higher = more creative
INFERNO_MODEL__TEMPERATURE=0.7

# =============================================================================
# MEMORY CONFIGURATION (Qdrant)
# =============================================================================

# Qdrant connection (start with: docker run -d -p 6333:6333 qdrant/qdrant)
INFERNO_MEMORY__QDRANT_HOST=localhost
INFERNO_MEMORY__QDRANT_PORT=6333
INFERNO_MEMORY__QDRANT_COLLECTION=inferno_memories

# For Qdrant Cloud (optional)
# INFERNO_MEMORY__QDRANT_API_KEY=your-qdrant-cloud-key

# Embedding provider for semantic memory
# Options:
#   - sentence_transformers  (FREE, local, default - recommended)
#   - ollama                 (FREE, local - requires Ollama running)
#   - openai                 (paid - requires OPENAI_API_KEY)
#   - voyage                 (paid - requires VOYAGE_API_KEY)
#   - cohere                 (paid - requires COHERE_API_KEY)
INFERNO_MEMORY__EMBEDDING_PROVIDER=sentence_transformers

# Ollama host (only if using ollama provider)
# INFERNO_MEMORY__OLLAMA_HOST=http://localhost:11434

# =============================================================================
# EXECUTION CONFIGURATION
# =============================================================================

# Maximum steps before stopping assessment
INFERNO_EXECUTION__MAX_STEPS=500

# Maximum conversation turns
INFERNO_EXECUTION__MAX_TURNS=500

# Total token budget before stopping
INFERNO_EXECUTION__MAX_TOTAL_TOKENS=1000000

# Enable streaming responses
INFERNO_EXECUTION__STREAM=true

# =============================================================================
# NETWORK CONFIGURATION
# =============================================================================

# Default HTTP timeout (seconds)
INFERNO_NETWORK__DEFAULT_TIMEOUT=30

# SSL verification (set to false only for testing)
INFERNO_NETWORK__VERIFY_SSL=true

# Rate limiting mode: fixed, adaptive, aggressive, stealth
INFERNO_NETWORK__RATE_LIMIT_MODE=adaptive

# Requests per second limit
INFERNO_NETWORK__REQUESTS_PER_SECOND=2.0

# Rotate user agents for OpSec
INFERNO_NETWORK__USER_AGENT_ROTATION=true

# =============================================================================
# OUTPUT CONFIGURATION
# =============================================================================

# Base directory for assessment outputs
INFERNO_OUTPUT__BASE_DIR=./outputs

# Report format: markdown, html, json
INFERNO_OUTPUT__REPORT_FORMAT=markdown

# Save tool output artifacts
INFERNO_OUTPUT__SAVE_ARTIFACTS=true

# Log level: DEBUG, INFO, WARNING, ERROR
INFERNO_OUTPUT__LOG_LEVEL=INFO

# =============================================================================
# EXTERNAL APIS (Optional but Recommended)
# =============================================================================

# NVD API Key (FREE - highly recommended)
# Get yours at: https://nvd.nist.gov/developers/request-an-api-key
# With key: 50 requests/30 sec | Without: 5 requests/30 sec
# NVD_API_KEY=your-nvd-api-key

# Shodan API (for passive reconnaissance)
# Get at: https://shodan.io
# SHODAN_API_KEY=your-shodan-key

# Censys API (for host/certificate search)
# Get at: https://search.censys.io/account/api
# CENSYS_API_ID=your-censys-id
# CENSYS_API_SECRET=your-censys-secret

# VirusTotal API (for file/URL analysis)
# Get at: https://www.virustotal.com/gui/my-apikey
# VIRUSTOTAL_API_KEY=your-vt-key

# SecurityTrails API (for DNS intelligence)
# Get at: https://securitytrails.com/corp/api
# SECURITYTRAILS_API_KEY=your-st-key

# GitHub Token (for repository reconnaissance)
# Create at: https://github.com/settings/tokens
# GITHUB_TOKEN=ghp_...

# =============================================================================
# CLOUD EMBEDDING PROVIDERS (Only if not using sentence_transformers)
# =============================================================================

# OpenAI API Key (for openai embedding provider)
# OPENAI_API_KEY=sk-...

# Voyage AI API Key (for voyage embedding provider - Anthropic recommended)
# VOYAGE_API_KEY=...

# Cohere API Key (for cohere embedding provider)
# COHERE_API_KEY=...

# =============================================================================
# OBSERVABILITY (Optional)
# =============================================================================

# Enable observability features
INFERNO_OBSERVABILITY__ENABLED=true

# Langfuse integration (optional tracing)
# INFERNO_OBSERVABILITY__LANGFUSE_ENABLED=false
# INFERNO_OBSERVABILITY__LANGFUSE_HOST=https://cloud.langfuse.com
# INFERNO_OBSERVABILITY__LANGFUSE_PUBLIC_KEY=pk-...
# INFERNO_OBSERVABILITY__LANGFUSE_SECRET_KEY=sk-...

# =============================================================================
# SECURITY GUARDRAILS
# =============================================================================

# Enable security guardrails (blocks dangerous commands, detects leaks)
INFERNO_GUARDRAILS=true

# =============================================================================
# DOCKER/MONITORING (for docker-compose with monitoring profile)
# =============================================================================

# Grafana admin password
# GRAFANA_PASSWORD=admin
