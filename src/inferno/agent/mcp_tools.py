"""
Custom MCP tools for Inferno.

This module provides Inferno-specific tools as MCP servers
that integrate with the Claude Agent SDK. These tools wrap
the existing Inferno infrastructure (Mem0/Qdrant for semantic memory).
"""

from __future__ import annotations

import asyncio
import uuid
from typing import Any

import structlog
from claude_agent_sdk import create_sdk_mcp_server, tool

logger = structlog.get_logger(__name__)

# Algorithm learning integration for spawn outcome tracking
try:
    from inferno.algorithms.integration import get_loop_integration
    ALGORITHM_LEARNING_AVAILABLE = True
except ImportError:
    ALGORITHM_LEARNING_AVAILABLE = False

# Global NVD tool instance
_nvd_tool_instance = None

# Global state for the memory tool instance
_memory_tool_instance = None
_operation_id: str | None = None
_current_target: str | None = None


def set_operation_id(op_id: str) -> None:
    """Set the current operation ID for memory storage."""
    global _operation_id, _memory_tool_instance
    _operation_id = op_id
    if _memory_tool_instance:
        _memory_tool_instance.set_operation_id(op_id)


def set_target(target: str) -> None:
    """Set the current target for memory scoping."""
    global _current_target, _memory_tool_instance
    _current_target = target
    if _memory_tool_instance:
        _memory_tool_instance.set_target(target)
    logger.info("mcp_memory_target_set", target=target)


def configure_memory(
    qdrant_host: str = "localhost",
    qdrant_port: int = 6333,
    qdrant_collection: str = "inferno_memories",
    embedding_provider: str = "sentence_transformers",
    embedding_model: str | None = None,
    ollama_host: str = "http://localhost:11434",
    api_key: str | None = None,
) -> None:
    """
    Configure the Mem0/Qdrant memory backend.

    This should be called before creating the MCP server to
    configure the semantic memory system.
    """
    global _memory_tool_instance, _operation_id

    from inferno.tools.memory import MemoryToolWithFallback

    _memory_tool_instance = MemoryToolWithFallback(
        operation_id=_operation_id,
        qdrant_host=qdrant_host,
        qdrant_port=qdrant_port,
        qdrant_collection=qdrant_collection,
        embedding_provider=embedding_provider,
        embedding_model=embedding_model,
        ollama_host=ollama_host,
        api_key=api_key,
    )

    logger.info(
        "mcp_memory_configured",
        qdrant_host=qdrant_host,
        embedding_provider=embedding_provider,
    )


def _get_memory_tool():
    """Get or create the memory tool instance."""
    global _memory_tool_instance, _operation_id, _current_target

    if _memory_tool_instance is None:
        from inferno.tools.memory import MemoryToolWithFallback

        _memory_tool_instance = MemoryToolWithFallback(
            operation_id=_operation_id,
        )
        # Set target if already configured
        if _current_target:
            _memory_tool_instance.set_target(_current_target)

    return _memory_tool_instance


# -------------------------------------------------------------------------
# Swarm Tool Configuration - for spawning subagents
# -------------------------------------------------------------------------

_swarm_model = "claude-opus-4-5-20251101"
_swarm_target = None
_swarm_configured = False
_max_subagents = 10  # Default limit
_current_subagent_count = 0  # Track spawned subagents


def configure_swarm(
    model: str = "claude-opus-4-5-20251101",
    target: str | None = None,
    max_subagents: int = 10,
) -> None:
    """
    Configure the swarm tool.

    Swarm tool now uses Claude SDK internally (supports OAuth automatically).

    Args:
        model: Model to use for subagents
        target: Target URL for context
        max_subagents: Maximum number of subagents to spawn
    """
    global _swarm_model, _swarm_target, _swarm_configured, _max_subagents, _current_subagent_count
    _swarm_model = model
    _swarm_target = target
    _swarm_configured = True
    _max_subagents = max_subagents
    _current_subagent_count = 0  # Reset on new configuration
    logger.info(
        "swarm_configured",
        model=model,
        target=target,
        max_subagents=max_subagents,
    )


def _get_swarm_tool():
    """Get or create the swarm tool instance."""
    global _swarm_model, _swarm_target, _operation_id, _swarm_configured

    if not _swarm_configured:
        # Auto-configure with defaults
        _swarm_configured = True

    from inferno.swarm.tool import SwarmTool

    return SwarmTool(
        model=_swarm_model,
        operation_id=_operation_id,
        target=_swarm_target,
    )


@tool(
    "memory_store",
    "Store a PROVEN finding or observation. "
    "IMPORTANT: For findings with severity, you MUST include proof of exploitation. "
    "Observable â‰  Exploitable - only store as 'finding' if you PROVED impact. "
    "Use 'observation' type for unproven discoveries that need more testing.",
    {
        "content": str,
        "memory_type": str,  # "finding" (proven), "observation" (unproven), "context", "credential"
        "severity": str,  # "critical", "high", "medium", "low" - ONLY for proven findings
        "proof": str,  # REQUIRED for findings: what you did to prove exploitation
        "metadata": str,  # JSON string of additional metadata
    }
)
async def memory_store(args: dict[str, Any]) -> dict[str, Any]:
    """Store information in Mem0/Qdrant semantic memory."""
    import json

    mem_tool = _get_memory_tool()

    # Parse metadata if provided as JSON string
    metadata = {}
    if args.get("metadata"):
        try:
            metadata = json.loads(args["metadata"])
        except json.JSONDecodeError:
            metadata = {"raw": args["metadata"]}

    # Add severity to metadata
    if args.get("severity"):
        metadata["severity"] = args["severity"]

    # Add proof to metadata
    if args.get("proof"):
        metadata["proof"] = args["proof"]

    # Key findings with severity should go to both episodic and semantic
    memory_type = args.get("memory_type", "findings")
    has_severity = bool(args.get("severity"))
    memory_scope = "both" if (has_severity or memory_type == "findings") else "episodic"

    result = await mem_tool.execute(
        operation="store",
        content=args["content"],
        memory_type=memory_type,
        memory_scope=memory_scope,  # Key findings go to BOTH for cross-session access
        metadata=metadata,
        target=_current_target,  # Pass global target for episodic memory
    )

    if result.success:
        return {
            "content": [{
                "type": "text",
                "text": result.output
            }]
        }
    else:
        return {
            "content": [{
                "type": "text",
                "text": f"Error storing memory: {result.error}"
            }],
            "is_error": True
        }


@tool(
    "memory_search",
    "Search stored memories using semantic similarity. "
    "Uses vector embeddings to find contextually relevant information, "
    "not just keyword matching. Use this to recall previously discovered findings.",
    {
        "query": str,
        "memory_type": str,  # "findings", "context", "knowledge", "checkpoint" or empty for all
        "limit": int,  # max results (default 10)
        "threshold": float,  # similarity threshold 0.0-1.0 (default 0.25 for good recall)
    }
)
async def memory_search(args: dict[str, Any]) -> dict[str, Any]:
    """Search Mem0/Qdrant semantic memory with similarity matching."""
    mem_tool = _get_memory_tool()

    # Default to 0.25 threshold for better recall of similar memories
    threshold = args.get("threshold", 0.25)

    result = await mem_tool.execute(
        operation="search",
        content=args["query"],
        memory_type=args.get("memory_type", "findings"),
        memory_scope="both",  # Search BOTH episodic (target-specific) and semantic (global)
        limit=args.get("limit", 10),
        threshold=threshold,
        target=_current_target,  # Pass global target for episodic memory
    )

    if result.success:
        return {
            "content": [{
                "type": "text",
                "text": result.output
            }]
        }
    else:
        return {
            "content": [{
                "type": "text",
                "text": f"Error searching memory: {result.error}"
            }],
            "is_error": True
        }


@tool(
    "memory_list",
    "List all stored memories of a specific type. "
    "Use to review what has been discovered so far.",
    {
        "memory_type": str,  # "findings", "context", "knowledge", "checkpoint"
        "limit": int,  # max results (default 10)
    }
)
async def memory_list(args: dict[str, Any]) -> dict[str, Any]:
    """List memories from Mem0/Qdrant."""
    mem_tool = _get_memory_tool()

    result = await mem_tool.execute(
        operation="list",
        memory_type=args.get("memory_type", "findings"),
        limit=args.get("limit", 10),
        target=_current_target,  # Pass global target for episodic memory
    )

    if result.success:
        return {
            "content": [{
                "type": "text",
                "text": result.output
            }]
        }
    else:
        return {
            "content": [{
                "type": "text",
                "text": f"Error listing memories: {result.error}"
            }],
            "is_error": True
        }


@tool(
    "memory_recall",
    "âš ï¸ DEPRECATED - DO NOT USE. Recall by ID is NOT supported in dual memory mode. "
    "Use memory_search instead to find memories by content.",
    {
        "memory_id": str,
    }
)
async def memory_recall(args: dict[str, Any]) -> dict[str, Any]:
    """Recall a specific memory by ID. DEPRECATED - use memory_search instead."""
    mem_tool = _get_memory_tool()

    result = await mem_tool.execute(
        operation="recall",
        memory_id=args["memory_id"],
    )

    if result.success:
        return {
            "content": [{
                "type": "text",
                "text": result.output
            }]
        }
    else:
        return {
            "content": [{
                "type": "text",
                "text": f"Error recalling memory: {result.error}"
            }],
            "is_error": True
        }


@tool(
    "checkpoint",
    "Create an assessment checkpoint to summarize progress. "
    "Use at 20%, 40%, 60%, and 80% budget consumption to track progress "
    "and ensure important findings are not lost.",
    {
        "phase": str,  # "reconnaissance", "scanning", "exploitation", "reporting"
        "summary": str,
        "findings_count": int,
        "next_steps": str,
        "progress_percent": int,  # 0-100
    }
)
async def checkpoint(args: dict[str, Any]) -> dict[str, Any]:
    """Create an assessment checkpoint in semantic memory."""
    mem_tool = _get_memory_tool()

    # Build checkpoint content
    content = (
        f"CHECKPOINT - {args['phase'].upper()} PHASE\n"
        f"Progress: {args.get('progress_percent', 0)}%\n"
        f"Summary: {args['summary']}\n"
        f"Findings so far: {args.get('findings_count', 0)}\n"
        f"Next steps: {args.get('next_steps', 'Continue assessment')}"
    )

    metadata = {
        "phase": args["phase"],
        "progress_percent": args.get("progress_percent", 0),
        "findings_count": args.get("findings_count", 0),
        "next_steps": args.get("next_steps"),
    }

    result = await mem_tool.execute(
        operation="checkpoint",
        content=content,
        metadata=metadata,
        target=_current_target,  # Pass target for episodic memory
    )

    if result.success:
        return {
            "content": [{
                "type": "text",
                "text": f"Checkpoint saved for {args['phase']} phase ({args.get('progress_percent', 0)}% complete)\n{result.output}"
            }]
        }
    else:
        return {
            "content": [{
                "type": "text",
                "text": f"Error creating checkpoint: {result.error}"
            }],
            "is_error": True
        }


# Global evaluation metrics
_evaluation_metrics = {
    "vulnerabilities_found": 0,
    "critical": 0,
    "high": 0,
    "medium": 0,
    "low": 0,
    "tools_used": [],
    "swarm_deployments": 0,
    "meta_tools_created": 0,
    "confidence_history": [],
}


@tool(
    "store_evidence",
    "Store a confirmed vulnerability finding with full evidence. "
    "Use this for ALL confirmed vulnerabilities. Automatically categorizes and tracks metrics.",
    {
        "vulnerability_type": str,  # "sqli", "xss", "rce", "ssrf", "idor", "auth_bypass", etc.
        "severity": str,  # "critical", "high", "medium", "low"
        "endpoint": str,  # The affected endpoint/URL
        "payload": str,  # The payload that triggered the vulnerability
        "evidence": str,  # Proof of exploitation (response, error, etc.)
        "cvss_score": float,  # CVSS score if known (0.0-10.0)
        "remediation": str,  # Suggested fix
    }
)
async def store_evidence(args: dict[str, Any]) -> dict[str, Any]:
    """Store a vulnerability finding with evidence."""
    # Update metrics first (always succeeds)
    global _evaluation_metrics
    _evaluation_metrics["vulnerabilities_found"] += 1
    severity = args.get("severity", "medium").lower()
    if severity in _evaluation_metrics:
        _evaluation_metrics[severity] += 1

    # Extract fields with defaults to avoid KeyError
    vuln_type = args.get("vulnerability_type", "Unknown")
    severity_str = args.get("severity", "medium")
    endpoint = args.get("endpoint", "Unknown")
    payload = args.get("payload", "N/A")
    evidence = args.get("evidence", "N/A")
    cvss = args.get("cvss_score", "N/A")
    remediation = args.get("remediation", "Not specified")

    # Build structured evidence content
    content = f"""VULNERABILITY: {vuln_type.upper()}
Severity: {severity_str.upper()}
Endpoint: {endpoint}
Payload: {payload}
Evidence: {evidence}
CVSS: {cvss}
Remediation: {remediation}"""

    # Try to store in memory (may fail due to Mem0 bugs, but metrics are already updated)
    try:
        mem_tool = _get_memory_tool()
        metadata = {
            "category": "vulnerability",
            "vulnerability_type": vuln_type,
            "severity": severity_str,
            "endpoint": endpoint,
            "cvss_score": cvss,
            "has_payload": bool(payload and payload != "N/A"),
            "has_evidence": bool(evidence and evidence != "N/A"),
        }

        result = await mem_tool.execute(
            operation="store",
            content=content,
            memory_type="finding",
            memory_scope="both",  # Store to BOTH episodic and semantic for key findings
            metadata=metadata,
            target=_current_target,  # CRITICAL: Pass target for episodic memory
        )

        if result.success:
            return {
                "content": [{
                    "type": "text",
                    "text": f"Evidence stored: {vuln_type.upper()} ({severity_str}) at {endpoint}\n"
                            f"Total vulnerabilities found: {_evaluation_metrics['vulnerabilities_found']}"
                }]
            }
    except Exception:
        # Memory storage failed but metrics are updated - don't crash
        pass

    # Return success even if memory failed (metrics were updated)
    return {
        "content": [{
            "type": "text",
            "text": f"Evidence recorded: {vuln_type.upper()} ({severity_str}) at {endpoint}\n"
                    f"Total vulnerabilities found: {_evaluation_metrics['vulnerabilities_found']}\n"
                    f"(Note: Memory storage may have partial persistence)"
        }]
    }


@tool(
    "update_confidence",
    "Update and track your current confidence level. "
    "Use this after each major action to maintain metacognitive awareness. "
    "The system will suggest strategy changes based on confidence trends.",
    {
        "confidence": int,  # 0-100
        "reason": str,  # Why this confidence level
        "next_action": str,  # What you plan to do next
    }
)
async def update_confidence(args: dict[str, Any]) -> dict[str, Any]:
    """Track confidence levels for metacognitive assessment."""
    global _evaluation_metrics

    confidence = args.get("confidence", 50)
    reason = args.get("reason", "")
    next_action = args.get("next_action", "")

    # Track confidence history
    _evaluation_metrics["confidence_history"].append({
        "confidence": confidence,
        "reason": reason,
    })

    # Generate strategy recommendation based on confidence
    if confidence >= 80:
        recommendation = "HIGH confidence - proceed with direct exploitation using specialized tools"
    elif confidence >= 50:
        recommendation = "MEDIUM confidence - validate findings with multiple approaches before exploitation"
    elif confidence >= 20:
        recommendation = "LOW confidence - consider deploying Task subagents for parallel exploration"
    else:
        recommendation = "VERY LOW confidence - STRONGLY recommend deploying swarm or creating meta-tools"

    # Check for declining confidence trend
    history = _evaluation_metrics["confidence_history"]
    if len(history) >= 3:
        recent = [h["confidence"] for h in history[-3:]]
        if all(recent[i] > recent[i+1] for i in range(len(recent)-1)):
            recommendation += "\nâš ï¸ WARNING: Confidence declining - consider changing approach"

    return {
        "content": [{
            "type": "text",
            "text": f"CONFIDENCE: {confidence}%\n"
                    f"Reason: {reason}\n"
                    f"Next action: {next_action}\n\n"
                    f"ðŸ“Š Recommendation: {recommendation}"
        }]
    }


@tool(
    "get_metrics",
    "Get current assessment metrics and performance evaluation. "
    "Use to review progress and adjust strategy.",
    {}
)
async def get_metrics(args: dict[str, Any]) -> dict[str, Any]:
    """Get assessment metrics."""
    global _evaluation_metrics

    history = _evaluation_metrics["confidence_history"]
    avg_confidence = sum(h["confidence"] for h in history) / len(history) if history else 0

    metrics_text = f"""ðŸ“Š ASSESSMENT METRICS

Vulnerabilities Found: {_evaluation_metrics['vulnerabilities_found']}
  - Critical: {_evaluation_metrics['critical']}
  - High: {_evaluation_metrics['high']}
  - Medium: {_evaluation_metrics['medium']}
  - Low: {_evaluation_metrics['low']}

Confidence Tracking:
  - Current: {history[-1]['confidence'] if history else 'N/A'}%
  - Average: {avg_confidence:.1f}%
  - Samples: {len(history)}

Operations:
  - Swarm Deployments: {_evaluation_metrics['swarm_deployments']}
  - Meta-tools Created: {_evaluation_metrics['meta_tools_created']}
  - Tools Used: {len(set(_evaluation_metrics['tools_used']))} unique
"""

    return {
        "content": [{
            "type": "text",
            "text": metrics_text
        }]
    }


@tool(
    "register_swarm",
    "Register that a swarm/subagent was deployed. "
    "Call this when using Task tool for parallel exploration.",
    {
        "task_description": str,
    }
)
async def register_swarm(args: dict[str, Any]) -> dict[str, Any]:
    """Register swarm deployment for metrics."""
    global _evaluation_metrics
    _evaluation_metrics["swarm_deployments"] += 1

    return {
        "content": [{
            "type": "text",
            "text": f"Swarm deployment #{_evaluation_metrics['swarm_deployments']} registered: {args['task_description']}"
        }]
    }


# Track background swarm tasks
_background_swarm_tasks: dict[str, asyncio.Task] = {}
_background_swarm_results: dict[str, dict] = {}


@tool(
    "swarm",
    "SPAWN WORKERS IN PARALLEL! Don't test manually - delegate to specialized sub-agents. "
    "CRITICAL: Spawn MULTIPLE workers simultaneously for each discovered endpoint, parameter, and vuln type. "
    "Use background=true to spawn and CONTINUE WORKING while workers run! "
    "Example: spawn 5 workers with background=true, then continue your own testing. "
    "AGENT TYPES: "
    "reconnaissance (enumeration, tech discovery) | "
    "scanner (vuln detection per endpoint) | "
    "exploiter (exploit confirmed vulns) | "
    "validator (independent finding verification) | "
    "waf_bypass (evade filters) | "
    "api_flow (API testing) | "
    "business_logic (logic flaws) | "
    "post_exploitation (privesc) | "
    "IOT: iot_scanner, firmware_analyst, memory_forensics, radio_analyst, reverse_engineer. "
    "SPAWN 5-10 WORKERS with background=true for maximum parallelism!",
    {
        "agent_type": str,  # reconnaissance, scanner, exploiter, validator, waf_bypass, api_flow, etc.
        "task": str,  # Specific task: "Test /login for SQLi, XSS, auth bypass"
        "context": str,  # Relevant findings and target info for the worker
        "max_turns": int,  # Maximum turns (default: 100, max: 200)
        "background": bool,  # If true, spawn in background and continue (default: false)
    }
)
async def swarm_spawn(args: dict[str, Any]) -> dict[str, Any]:
    """
    Spawn a specialized sub-agent to handle a specific task.

    This is the META-AGENT pattern - the main agent can spawn specialized
    workers for parallel task execution.

    When background=true, spawns the worker and returns immediately so the
    main agent can continue working. Use swarm_status to check on workers.
    """
    global _evaluation_metrics, _background_swarm_tasks, _background_swarm_results, _current_subagent_count, _max_subagents

    # Check subagent limit
    if _current_subagent_count >= _max_subagents:
        return {
            "content": [{
                "type": "text",
                "text": f"LIMIT REACHED: Maximum {_max_subagents} subagents allowed. Currently spawned: {_current_subagent_count}. Focus on manual testing or wait for existing workers to complete."
            }],
            "is_error": False,  # Not an error, just a limit
        }

    swarm_tool = _get_swarm_tool()
    if swarm_tool is None:
        return {
            "content": [{
                "type": "text",
                "text": "ERROR: Swarm tool not configured. Cannot spawn subagents."
            }],
            "is_error": True,
        }

    # Track deployment
    _evaluation_metrics["swarm_deployments"] += 1
    _current_subagent_count += 1

    agent_type = args.get("agent_type", "reconnaissance")
    task = args.get("task", "")
    context = args.get("context", "")
    max_turns = min(args.get("max_turns", 100), 200)
    background = args.get("background", False)

    logger.info(
        "spawning_subagent_via_mcp",
        agent_type=agent_type,
        task=task[:100],
        max_turns=max_turns,
        background=background,
    )

    async def run_worker():
        """Execute worker and store result."""
        try:
            result = await swarm_tool.execute(
                agent_type=agent_type,
                task=task,
                context=context,
                max_turns=max_turns,
            )

            # Record spawn outcome for algorithm learning
            if ALGORITHM_LEARNING_AVAILABLE:
                try:
                    loop_integration = get_loop_integration()
                    # Count findings from output (rough heuristic)
                    findings_count = 0
                    if result.output:
                        output_lower = result.output.lower()
                        if 'finding' in output_lower or 'vulnerability' in output_lower:
                            findings_count = output_lower.count('finding') + output_lower.count('vulnerability')
                        if 'flag' in output_lower:
                            findings_count += output_lower.count('flag{')
                    # Estimate tokens: ~4 chars per token, assume output reflects usage
                    estimated_tokens = len(result.output or '') // 4 if result.output else 0
                    loop_integration.record_spawn_outcome(
                        success=result.success,
                        findings_count=findings_count,
                        turns_used=max_turns,
                        tokens_used=estimated_tokens,
                    )
                except Exception as e:
                    logger.warning("spawn_outcome_recording_failed", error=str(e))

            return {
                "success": result.success,
                "output": result.output,
                "error": result.error,
                "metadata": result.metadata,
            }
        except Exception as e:
            # Record failed spawn for learning
            if ALGORITHM_LEARNING_AVAILABLE:
                try:
                    loop_integration = get_loop_integration()
                    loop_integration.record_spawn_outcome(
                        success=False,
                        findings_count=0,
                        turns_used=0,
                        tokens_used=0,
                    )
                except Exception:
                    pass
            return {"success": False, "error": str(e)}

    if background:
        # Spawn in background and return immediately
        worker_id = f"{agent_type}_{uuid.uuid4().hex[:8]}"
        task_obj = asyncio.create_task(run_worker())
        _background_swarm_tasks[worker_id] = task_obj

        # Set up callback to store result when done
        def store_result(t):
            try:
                _background_swarm_results[worker_id] = t.result()
            except Exception as e:
                _background_swarm_results[worker_id] = {"success": False, "error": str(e)}

        task_obj.add_done_callback(store_result)

        return {
            "content": [{
                "type": "text",
                "text": f"âœ“ Worker '{worker_id}' spawned in background.\n"
                        f"  Type: {agent_type}\n"
                        f"  Task: {task[:80]}...\n"
                        f"  Max turns: {max_turns}\n\n"
                        f"Continue working! Use swarm_status to check progress."
            }]
        }

    # Foreground (blocking) mode
    try:
        result = await swarm_tool.execute(
            agent_type=agent_type,
            task=task,
            context=context,
            max_turns=max_turns,
        )

        # Record spawn outcome for algorithm learning
        if ALGORITHM_LEARNING_AVAILABLE:
            try:
                loop_integration = get_loop_integration()
                findings_count = 0
                if result.output:
                    output_lower = result.output.lower()
                    if 'finding' in output_lower or 'vulnerability' in output_lower:
                        findings_count = output_lower.count('finding') + output_lower.count('vulnerability')
                    if 'flag' in output_lower:
                        findings_count += output_lower.count('flag{')
                # Estimate tokens: ~4 chars per token
                estimated_tokens = len(result.output or '') // 4 if result.output else 0
                loop_integration.record_spawn_outcome(
                    success=result.success,
                    findings_count=findings_count,
                    turns_used=max_turns,
                    tokens_used=estimated_tokens,
                )
            except Exception as e:
                logger.warning("spawn_outcome_recording_failed", error=str(e))

        if result.success:
            return {
                "content": [{
                    "type": "text",
                    "text": result.output or "Subagent completed successfully"
                }]
            }
        else:
            return {
                "content": [{
                    "type": "text",
                    "text": f"Subagent error: {result.error}"
                }],
                "is_error": True,
            }

    except Exception as e:
        # Record failed spawn for learning
        if ALGORITHM_LEARNING_AVAILABLE:
            try:
                loop_integration = get_loop_integration()
                loop_integration.record_spawn_outcome(
                    success=False,
                    findings_count=0,
                    turns_used=0,
                    tokens_used=0,
                )
            except Exception:
                pass
        logger.error("swarm_spawn_error", error=str(e))
        return {
            "content": [{
                "type": "text",
                "text": f"Failed to spawn subagent: {e}"
            }],
            "is_error": True,
        }


@tool(
    "swarm_status",
    "Check status of background swarm workers. Shows running workers and completed results. "
    "Use this to monitor progress of workers spawned with background=true.",
    {
        "worker_id": str,  # Optional: specific worker ID to check (leave empty for all)
    }
)
async def swarm_status(args: dict[str, Any]) -> dict[str, Any]:
    """Check status of background swarm workers."""
    global _background_swarm_tasks, _background_swarm_results

    worker_id = args.get("worker_id", "")

    if worker_id:
        # Check specific worker
        if worker_id in _background_swarm_results:
            result = _background_swarm_results[worker_id]
            return {
                "content": [{
                    "type": "text",
                    "text": f"Worker '{worker_id}' COMPLETED:\n"
                            f"Success: {result.get('success', False)}\n"
                            f"Output: {result.get('output', 'N/A')[:500]}...\n"
                            f"Error: {result.get('error', 'None')}"
                }]
            }
        elif worker_id in _background_swarm_tasks:
            task = _background_swarm_tasks[worker_id]
            status = "RUNNING" if not task.done() else "DONE"
            return {
                "content": [{
                    "type": "text",
                    "text": f"Worker '{worker_id}' status: {status}"
                }]
            }
        else:
            return {
                "content": [{
                    "type": "text",
                    "text": f"Worker '{worker_id}' not found."
                }],
                "is_error": True,
            }

    # Show all workers
    lines = ["## Background Swarm Workers\n"]

    running = []
    completed = []

    for wid, task in _background_swarm_tasks.items():
        if task.done():
            completed.append(wid)
        else:
            running.append(wid)

    if running:
        lines.append(f"**Running ({len(running)}):**")
        for wid in running:
            lines.append(f"  - {wid}")

    if completed:
        lines.append(f"\n**Completed ({len(completed)}):**")
        for wid in completed:
            result = _background_swarm_results.get(wid, {})
            status = "âœ“" if result.get("success") else "âœ—"
            lines.append(f"  - {wid}: {status}")

    if not running and not completed:
        lines.append("No background workers spawned yet.")
        lines.append("Use swarm(..., background=true) to spawn workers!")

    return {
        "content": [{
            "type": "text",
            "text": "\n".join(lines)
        }]
    }


# -------------------------------------------------------------------------
# Strategy & Scoring Tools - Algorithm-driven decision making
# -------------------------------------------------------------------------


@tool(
    "get_strategy",
    "Get AI-powered strategic recommendations using Q-Learning and multi-armed bandits. "
    "CRITICAL: Use this BEFORE deciding what to do next! Returns ranked actions with Q-values. "
    "The algorithm learns from your successes and failures to guide optimal attack selection.",
    {
        "current_phase": str,  # reconnaissance, scanning, exploitation, post_exploitation, reporting
        "endpoints_found": int,  # Number of endpoints discovered
        "vulns_found": int,  # Number of vulnerabilities found
        "shell_obtained": bool,  # Whether shell access has been obtained
        "tech_stack": str,  # Comma-separated: "php,mysql,apache"
    }
)
async def get_strategy(args: dict[str, Any]) -> dict[str, Any]:
    """Get Q-learning based strategic recommendations."""
    from inferno.tools.strategy import GetStrategyTool

    tool_instance = GetStrategyTool()
    tech_list = []
    if args.get("tech_stack"):
        tech_list = [t.strip() for t in args["tech_stack"].split(",")]

    result = await tool_instance.execute(
        current_phase=args.get("current_phase", "reconnaissance"),
        endpoints_found=args.get("endpoints_found", 0),
        vulns_found=args.get("vulns_found", 0),
        shell_obtained=args.get("shell_obtained", False),
        tech_stack=tech_list,
    )

    return {
        "content": [{"type": "text", "text": result.output}]
    }


@tool(
    "record_failure",
    "Record a FAILED attack attempt to learn from mistakes! "
    "CRITICAL: Call this EVERY time an attack fails! After 3 consecutive failures, "
    "that attack pattern is BLOCKED and you must try different approach. "
    "This feeds the Q-learning algorithm to avoid repeating mistakes.",
    {
        "endpoint": str,  # The endpoint tested (URL path)
        "attack_type": str,  # sqli, xss, ssti, lfi, rfi, ssrf, auth_bypass, rce, xxe, other
        "reason": str,  # waf_blocked, timeout, 403, no_vuln, rate_limited, etc.
        "payload": str,  # The payload that was used (optional)
    }
)
async def record_failure(args: dict[str, Any]) -> dict[str, Any]:
    """Record a failed attack to learn from mistakes."""
    from inferno.tools.strategy import RecordFailureTool

    tool_instance = RecordFailureTool()
    result = await tool_instance.execute(
        endpoint=args.get("endpoint", ""),
        attack_type=args.get("attack_type", "other"),
        reason=args.get("reason", "unknown"),
        payload=args.get("payload"),
    )

    return {
        "content": [{"type": "text", "text": result.output}]
    }


@tool(
    "record_success",
    "Record a SUCCESSFUL attack to reinforce learning! "
    "CRITICAL: Call this when a vulnerability is CONFIRMED or EXPLOITED. "
    "This updates Q-learning weights to favor successful techniques. "
    "Set exploited=true for FULL POINTS, otherwise you get 20% PENALTY!",
    {
        "endpoint": str,  # The vulnerable endpoint
        "attack_type": str,  # sqli, xss, ssti, lfi, rfi, ssrf, auth_bypass, rce, xxe, other
        "severity": str,  # critical, high, medium, low, info
        "exploited": bool,  # TRUE = full points, FALSE = 20% penalty!
    }
)
async def record_success(args: dict[str, Any]) -> dict[str, Any]:
    """Record a successful attack to reinforce learning."""
    from inferno.tools.strategy import RecordSuccessTool

    tool_instance = RecordSuccessTool()
    result = await tool_instance.execute(
        endpoint=args.get("endpoint", ""),
        attack_type=args.get("attack_type", "other"),
        severity=args.get("severity", "medium"),
        exploited=args.get("exploited", False),
    )

    # Add explicit scoring feedback
    exploited = args.get("exploited", False)
    if not exploited:
        result_text = result.output + "\n\nâš ï¸ WARNING: VERIFIED but NOT EXPLOITED = 20% PENALTY!\nSpawn an 'exploiter' worker to get FULL POINTS!"
    else:
        result_text = result.output + "\n\nâœ“ EXPLOITED = FULL POINTS! Well done."

    return {
        "content": [{"type": "text", "text": result_text}]
    }


@tool(
    "get_scoring",
    "Show current scoring with 20% PENALTY calculation for non-exploited findings. "
    "Use this to understand why you MUST exploit findings, not just detect them!",
    {
        "detection_complexity": float,  # DC score (1-10)
        "exploit_complexity": float,  # EC score (1-10)
    }
)
async def get_scoring(args: dict[str, Any]) -> dict[str, Any]:
    """Calculate and display scoring with penalty."""
    dc = args.get("detection_complexity", 5.0)
    ec = args.get("exploit_complexity", 8.0)

    exploited_score = dc + ec
    verified_score = dc + (ec * 0.8)
    penalty = exploited_score - verified_score

    output = f"""## SCORING SYSTEM - 20% Penalty for Non-Exploitation

### Your Finding Scores (DC={dc}, EC={ec})

| Status | Formula | Score |
|--------|---------|-------|
| **EXPLOITED** | TC = DC + EC | **{exploited_score:.1f}** âœ“ |
| VERIFIED | TC = DC + ECÃ—0.8 | {verified_score:.1f} |

**You lose {penalty:.1f} points if you don't exploit!**

### How to Get Full Points
1. Don't just DETECT vulnerabilities - EXPLOIT them!
2. Spawn `exploiter` workers for each finding
3. Achieve actual impact: data extraction, RCE, auth bypass

### Action Required
```
swarm(agent_type="exploiter", task="Fully exploit [your finding]", background=true)
```

Remember: Verified-only findings are PENALIZED 20%!"""

    return {
        "content": [{"type": "text", "text": output}]
    }


@tool(
    "get_swarm_plan",
    "Generate a comprehensive plan for spawning MULTIPLE sub-agents in PARALLEL! "
    "Use this when you have multiple endpoints, vulns, or targets to test. "
    "Returns executable swarm commands - COPY AND EXECUTE THEM!",
    {
        "endpoints": str,  # Comma-separated: "/login,/api/users,/search"
        "vulns_to_exploit": str,  # Comma-separated: "SQLi in /search,XSS in /comment"
        "subdomains": str,  # Comma-separated: "api.target.com,admin.target.com"
        "max_parallel": int,  # Max parallel agents (default 5)
    }
)
async def get_swarm_plan(args: dict[str, Any]) -> dict[str, Any]:
    """Generate parallel swarm execution plan."""
    from inferno.tools.strategy import GetSwarmPlanTool

    # Parse comma-separated strings to lists
    endpoints = [e.strip() for e in args.get("endpoints", "").split(",") if e.strip()]
    vulns = [v.strip() for v in args.get("vulns_to_exploit", "").split(",") if v.strip()]
    subdomains = [s.strip() for s in args.get("subdomains", "").split(",") if s.strip()]

    tool_instance = GetSwarmPlanTool()
    result = await tool_instance.execute(
        endpoints=endpoints,
        vulns_to_exploit=vulns,
        subdomains=subdomains,
        max_parallel=args.get("max_parallel", 5),
    )

    return {
        "content": [{"type": "text", "text": result.output}]
    }


@tool(
    "nvd_lookup",
    "Query NVD for known CVEs when you detect ANY software version. "
    "CRITICAL: Use this IMMEDIATELY when you see version strings like 'nginx/1.18.0', "
    "'Express', 'WordPress 6.4.1', 'Apache/2.4.41', etc. Returns CVEs sorted by severity "
    "with exploit availability indicators.",
    {
        "software": str,  # Software name (e.g., "nginx", "wordpress", "express")
        "version": str,  # Version string (e.g., "1.18.0", "6.4.1")
        "auto_detect": str,  # OR raw version string to auto-detect (e.g., "nginx/1.18.0")
    }
)
async def nvd_lookup(args: dict[str, Any]) -> dict[str, Any]:
    """Query NVD for known CVEs for a software/version."""
    global _nvd_tool_instance

    # Lazy load NVD tool
    if _nvd_tool_instance is None:
        from inferno.tools.security.nvd import NVDTool
        _nvd_tool_instance = NVDTool()

    try:
        result = await _nvd_tool_instance.execute(
            software=args.get("software"),
            version=args.get("version"),
            auto_detect=args.get("auto_detect"),
        )

        if result.success:
            return {
                "content": [{
                    "type": "text",
                    "text": result.output
                }]
            }
        else:
            return {
                "content": [{
                    "type": "text",
                    "text": f"NVD lookup failed: {result.error}"
                }],
                "is_error": True
            }
    except Exception as e:
        logger.error("nvd_lookup_error", error=str(e))
        return {
            "content": [{
                "type": "text",
                "text": f"NVD lookup error: {e!s}"
            }],
            "is_error": True
        }


# Global Caido tool instance
_caido_tool_instance = None


@tool(
    "caido",
    "Interact with Caido web security proxy for traffic inspection and replay. "
    "Use 'setup' operation at the START of an assessment to auto-authenticate (guest login) "
    "and create a Caido project - NO TOKEN REQUIRED if Caido has --allow-guests enabled. "
    "Operations: status, setup, get_requests, get_request, replay, search. "
    "After setup, route HTTP requests through Caido proxy for traffic capture.",
    {
        "operation": str,  # status, setup, get_requests, get_request, replay, search
        "request_id": str,  # Request ID for get_request and replay operations
        "host_filter": str,  # Filter requests by host
        "httpql": str,  # HTTPQL query (e.g., "req.method.eq:POST", "resp.status.eq:200")
        "limit": int,  # Maximum results (default: 20)
        "modifications": dict,  # Modifications for replay (headers, body, etc.)
        "assessment_name": str,  # Project name for 'setup' operation
    }
)
async def caido_tool(args: dict[str, Any]) -> dict[str, Any]:
    """Interact with Caido web security proxy."""
    global _caido_tool_instance

    # Lazy load Caido tool
    if _caido_tool_instance is None:
        try:
            from inferno.tools.caido import CaidoTool
            _caido_tool_instance = CaidoTool()
        except ImportError as e:
            return {
                "content": [{
                    "type": "text",
                    "text": f"Caido tool not available: {e}. Install httpx: pip install httpx"
                }],
                "is_error": True
            }

    try:
        result = await _caido_tool_instance.execute(
            operation=args.get("operation", "status"),
            request_id=args.get("request_id"),
            host_filter=args.get("host_filter"),
            httpql=args.get("httpql"),
            limit=args.get("limit", 20),
            modifications=args.get("modifications"),
            assessment_name=args.get("assessment_name"),
        )

        if result.success:
            return {
                "content": [{
                    "type": "text",
                    "text": result.output
                }]
            }
        else:
            return {
                "content": [{
                    "type": "text",
                    "text": f"Caido operation failed: {result.error}"
                }],
                "is_error": True
            }
    except Exception as e:
        logger.error("caido_tool_error", error=str(e))
        return {
            "content": [{
                "type": "text",
                "text": f"Caido error: {e!s}"
            }],
            "is_error": True
        }


# -------------------------------------------------------------------------
# Core Testing Tools (MCP wrappers for sub-agents)
# -------------------------------------------------------------------------
# These wrap the core Inferno tools so sub-agents can access them via MCP

@tool(
    "http_request",
    "Make HTTP requests to test web applications. Supports all HTTP methods, "
    "headers, body data, cookies, and proxy configuration. Use for testing "
    "endpoints, sending payloads, and analyzing responses. "
    "Pass headers/cookies/json_body as JSON strings, e.g., '{\"Authorization\": \"Bearer token\"}'",
    {
        "url": str,  # Target URL (required)
        "method": str,  # HTTP method: GET, POST, PUT, DELETE, PATCH, OPTIONS, HEAD (default: GET)
        "headers": str,  # Custom headers as JSON string, e.g., '{"Authorization": "Bearer x"}'
        "body": str,  # Request body (for POST, PUT, PATCH)
        "json_body": str,  # JSON body as string (auto-sets Content-Type)
        "cookies": str,  # Cookies as JSON string, e.g., '{"session": "abc123"}'
        "timeout": int,  # Request timeout in seconds (default: 30)
        "follow_redirects": bool,  # Follow redirects (default: true)
        "proxy": str,  # Proxy URL (e.g., http://localhost:8080 for Caido)
    }
)
async def http_request_tool(args: dict[str, Any]) -> dict[str, Any]:
    """Make HTTP requests via MCP for sub-agents."""
    try:
        import json as json_module

        from inferno.tools.http import HTTPTool

        # Parse JSON string parameters
        headers = None
        if args.get("headers"):
            try:
                headers = json_module.loads(args["headers"]) if isinstance(args["headers"], str) else args["headers"]
            except json_module.JSONDecodeError:
                pass

        cookies = None
        if args.get("cookies"):
            try:
                cookies = json_module.loads(args["cookies"]) if isinstance(args["cookies"], str) else args["cookies"]
            except json_module.JSONDecodeError:
                pass

        json_body = None
        if args.get("json_body"):
            try:
                json_body = json_module.loads(args["json_body"]) if isinstance(args["json_body"], str) else args["json_body"]
            except json_module.JSONDecodeError:
                pass

        http_tool = HTTPTool()
        result = await http_tool.execute(
            url=args.get("url", ""),
            method=args.get("method", "GET"),
            headers=headers,
            body=args.get("body"),
            json_body=json_body,
            cookies=cookies,
            timeout=args.get("timeout", 30),
            follow_redirects=args.get("follow_redirects", True),
            proxy=args.get("proxy"),
        )

        if result.success:
            return {
                "content": [{
                    "type": "text",
                    "text": result.output
                }]
            }
        else:
            return {
                "content": [{
                    "type": "text",
                    "text": f"HTTP request failed: {result.error}"
                }],
                "is_error": True
            }
    except Exception as e:
        logger.error("http_request_tool_error", error=str(e))
        return {
            "content": [{
                "type": "text",
                "text": f"HTTP request error: {e!s}"
            }],
            "is_error": True
        }


@tool(
    "execute_command",
    "Execute shell commands for security testing. Supports running tools like "
    "nmap, gobuster, sqlmap, nuclei, curl, etc. Commands run in a sandbox. "
    "Use for reconnaissance, scanning, and exploitation.",
    {
        "command": str,  # Command to execute (required)
        "timeout": int,  # Timeout in seconds (default: 120)
        "working_dir": str,  # Working directory for command execution
    }
)
async def execute_command_tool(args: dict[str, Any]) -> dict[str, Any]:
    """Execute commands via MCP for sub-agents."""
    try:
        from inferno.tools.execute_command import execute_command

        # execute_command is a FunctionTool, call its .execute() method
        result = await execute_command.execute(
            command=args.get("command", ""),
            timeout=args.get("timeout", 120),
            working_dir=args.get("working_dir"),
        )

        if result.success:
            return {
                "content": [{
                    "type": "text",
                    "text": result.output
                }]
            }
        else:
            return {
                "content": [{
                    "type": "text",
                    "text": f"Command failed: {result.error}\nOutput: {result.output}"
                }],
                "is_error": True
            }
    except Exception as e:
        logger.error("execute_command_tool_error", error=str(e))
        return {
            "content": [{
                "type": "text",
                "text": f"Command execution error: {e!s}"
            }],
            "is_error": True
        }


@tool(
    "think",
    "Structured reasoning tool for analysis and decision-making. Use this BEFORE "
    "complex decisions to organize thoughts. Records reasoning for learning. "
    "Types: analysis, hypothesis, planning, reflection, decision, breakthrough.",
    {
        "thought": str,  # Your reasoning/analysis (required)
        "thought_type": str,  # Type: analysis, hypothesis, planning, reflection, decision, breakthrough
        "context": str,  # Additional context (findings, errors, etc.)
    }
)
async def think_tool(args: dict[str, Any]) -> dict[str, Any]:
    """Structured reasoning via MCP for sub-agents."""
    try:
        from inferno.tools.think import ThinkTool

        think = ThinkTool()
        result = await think.execute(
            thought=args.get("thought", ""),
            thought_type=args.get("thought_type", "analysis"),
            context=args.get("context"),
        )

        if result.success:
            return {
                "content": [{
                    "type": "text",
                    "text": result.output
                }]
            }
        else:
            return {
                "content": [{
                    "type": "text",
                    "text": f"Think failed: {result.error}"
                }],
                "is_error": True
            }
    except Exception as e:
        logger.error("think_tool_error", error=str(e))
        return {
            "content": [{
                "type": "text",
                "text": f"Think error: {e!s}"
            }],
            "is_error": True
        }


@tool(
    "register_meta_tool",
    "Register that a custom meta-tool was created. "
    "Call this after creating a custom script/tool.",
    {
        "tool_name": str,
        "tool_path": str,
        "description": str,
    }
)
async def register_meta_tool(args: dict[str, Any]) -> dict[str, Any]:
    """Register meta-tool creation for metrics."""
    global _evaluation_metrics
    _evaluation_metrics["meta_tools_created"] += 1

    # Also store in memory for cross-session reuse
    mem_tool = _get_memory_tool()
    await mem_tool.execute(
        operation="store",
        content=f"META-TOOL: {args['tool_name']}\nPath: {args['tool_path']}\nDescription: {args['description']}",
        memory_type="knowledge",
        metadata={"category": "meta_tool", "tool_name": args["tool_name"]},
    )

    return {
        "content": [{
            "type": "text",
            "text": f"Meta-tool #{_evaluation_metrics['meta_tools_created']} registered: {args['tool_name']} at {args['tool_path']}"
        }]
    }


# -------------------------------------------------------------------------
# Key Findings Persistence (CAI-inspired)
# -------------------------------------------------------------------------
# Simple file-based persistence for maintaining findings across turns
# This prevents the agent from "forgetting" important discoveries

_key_findings_file: str | None = None


def set_key_findings_file(filepath: str) -> None:
    """Set the path for key findings persistence file."""
    global _key_findings_file
    _key_findings_file = filepath


def _get_findings_file() -> str:
    """Get the key findings file path, creating default if needed."""
    global _key_findings_file
    if _key_findings_file is None:
        import os
        import tempfile
        # Use temp dir with operation ID if available
        if _operation_id:
            _key_findings_file = os.path.join(tempfile.gettempdir(), f"inferno_findings_{_operation_id}.txt")
        else:
            _key_findings_file = os.path.join(tempfile.gettempdir(), "inferno_findings.txt")
    return _key_findings_file


@tool(
    "write_key_findings",
    "Write key findings to persistent state file. Use this to record important discoveries "
    "that MUST NOT be forgotten: confirmed vulnerabilities, credentials, attack paths, "
    "or any critical information. This persists across conversation turns and prevents "
    "context loss. Call this EVERY TIME you discover something important.",
    {
        "content": str,  # The findings to write (will be appended)
        "overwrite": bool,  # If True, replace entire file. If False (default), append.
    }
)
async def write_key_findings(args: dict[str, Any]) -> dict[str, Any]:
    """Write key findings to persistent file (CAI-inspired state.txt)."""
    filepath = _get_findings_file()
    content = args.get("content", "")
    overwrite = args.get("overwrite", False)

    if not content:
        return {
            "content": [{
                "type": "text",
                "text": "Error: No content provided to write"
            }],
            "is_error": True
        }

    try:
        import os
        from datetime import datetime

        # Add timestamp to entry
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        entry = f"\n[{timestamp}]\n{content}\n{'='*50}"

        mode = "w" if overwrite else "a"
        with open(filepath, mode, encoding="utf-8") as f:
            if overwrite:
                f.write(f"# INFERNO KEY FINDINGS\n# Operation: {_operation_id or 'unknown'}\n{'='*50}")
            f.write(entry)

        # Get file size for feedback
        size = os.path.getsize(filepath)

        return {
            "content": [{
                "type": "text",
                "text": f"Key findings written to state file ({size} bytes)\n"
                        f"File: {filepath}\n"
                        f"Mode: {'overwrite' if overwrite else 'append'}"
            }]
        }
    except Exception as e:
        logger.error("write_key_findings_error", error=str(e))
        return {
            "content": [{
                "type": "text",
                "text": f"Error writing findings: {e!s}"
            }],
            "is_error": True
        }


@tool(
    "read_key_findings",
    "Read all key findings from persistent state file. Use this to recall important "
    "discoveries from earlier in the assessment. Call this at the START of each major "
    "phase to refresh your context on what has been found.",
    {}
)
async def read_key_findings(args: dict[str, Any]) -> dict[str, Any]:
    """Read key findings from persistent file (CAI-inspired state.txt)."""
    filepath = _get_findings_file()

    try:
        import os

        if not os.path.exists(filepath):
            return {
                "content": [{
                    "type": "text",
                    "text": "No key findings recorded yet. Use write_key_findings to persist important discoveries."
                }]
            }

        with open(filepath, encoding="utf-8") as f:
            content = f.read()

        if not content.strip():
            return {
                "content": [{
                    "type": "text",
                    "text": "Key findings file is empty. Use write_key_findings to persist important discoveries."
                }]
            }

        # Count entries
        entry_count = content.count("[20")  # Rough count of timestamped entries

        return {
            "content": [{
                "type": "text",
                "text": f"=== KEY FINDINGS ({entry_count} entries) ===\n\n{content}"
            }]
        }
    except Exception as e:
        logger.error("read_key_findings_error", error=str(e))
        return {
            "content": [{
                "type": "text",
                "text": f"Error reading findings: {e!s}"
            }],
            "is_error": True
        }


def reset_metrics() -> None:
    """Reset evaluation metrics for a new assessment."""
    global _evaluation_metrics
    _evaluation_metrics = {
        "vulnerabilities_found": 0,
        "critical": 0,
        "high": 0,
        "medium": 0,
        "low": 0,
        "tools_used": [],
        "swarm_deployments": 0,
        "meta_tools_created": 0,
        "confidence_history": [],
    }


def get_final_metrics() -> dict:
    """Get final metrics for reporting."""
    global _evaluation_metrics
    history = _evaluation_metrics["confidence_history"]
    avg_confidence = sum(h["confidence"] for h in history) / len(history) if history else 0

    return {
        **_evaluation_metrics,
        "avg_confidence": avg_confidence,
    }


def create_inferno_mcp_server():
    """
    Create the Inferno MCP server with semantic memory and metacognitive tools.

    Includes:
    - Memory tools (store, search, list)
    - Evidence collection (store_evidence)
    - Metacognitive tools (update_confidence, get_metrics)
    - Swarm/meta-tool registration

    Returns:
        McpSdkServerConfig for use with ClaudeAgentOptions.
    """
    # Reset metrics for new session
    reset_metrics()

    return create_sdk_mcp_server(
        name="inferno",
        version="1.0.0",
        tools=[
            # Core Testing Tools (for sub-agents)
            http_request_tool,
            execute_command_tool,
            think_tool,
            # Strategy & Algorithm Tools (Q-Learning, Bandits, 20% Penalty)
            get_strategy,
            record_failure,
            record_success,
            get_scoring,
            get_swarm_plan,
            # Memory tools
            memory_store,
            memory_search,
            memory_list,
            # memory_recall - REMOVED: not supported in dual memory mode, use memory_search
            checkpoint,
            # Evidence collection
            store_evidence,
            # Key findings persistence (CAI-inspired)
            write_key_findings,
            read_key_findings,
            # Intelligence tools
            nvd_lookup,
            caido_tool,  # Caido proxy integration
            # Metacognitive tools
            update_confidence,
            get_metrics,
            # Swarm - spawn subagents (META-AGENT PATTERN)
            swarm_spawn,
            swarm_status,
            # Swarm/meta-tool registration (metrics only)
            register_swarm,
            register_meta_tool,
        ]
    )
